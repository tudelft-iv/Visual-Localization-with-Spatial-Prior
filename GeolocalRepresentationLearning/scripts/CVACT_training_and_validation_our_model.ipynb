{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.spatial import KDTree\n",
    "import copy\n",
    "import scipy\n",
    "from spatial_net import *\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.gen_math_ops import *\n",
    "import tensorflow_probability as tfp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputData:\n",
    "\n",
    "    sig = 25\n",
    "    \n",
    "    def __init__(self, radius):\n",
    "        # The radius defines the local neighborhood during trainin and testing\n",
    "        print('radius of the circle =', radius)        \n",
    "        self.radius = radius\n",
    "        \n",
    "        # put your path to the dataset here\n",
    "        self.image_root = '/local/zxia/datasets'\n",
    "        \n",
    "        # load the training, validation, and test set\n",
    "        trainlist = []\n",
    "        with open('./trainlist.txt', 'r') as filehandle:\n",
    "            filecontents = filehandle.readlines()\n",
    "            for line in filecontents:\n",
    "                # remove linebreak which is the last character of the string\n",
    "                content = line[:-1]\n",
    "                trainlist.append(content.split(\" \"))\n",
    "                \n",
    "        vallist = []\n",
    "        with open('./vallist.txt', 'r') as filehandle:\n",
    "            filecontents = filehandle.readlines()\n",
    "            for line in filecontents:\n",
    "                # remove linebreak which is the last character of the string\n",
    "                content = line[:-1]\n",
    "                vallist.append(content.split(\" \"))\n",
    "                \n",
    "        testlist = []\n",
    "        with open('./testlist.txt', 'r') as filehandle:\n",
    "            filecontents = filehandle.readlines()\n",
    "            for line in filecontents:\n",
    "                # remove linebreak which is the last character of the string\n",
    "                content = line[:-1]\n",
    "                testlist.append(content.split(\" \"))\n",
    "                \n",
    "        \n",
    "        self.trainList = trainlist\n",
    "        self.trainNum = len(trainlist)\n",
    "        trainarray = np.array(trainlist)\n",
    "        self.trainUTM = np.transpose(trainarray[:,2:].astype(np.float64))\n",
    "        \n",
    "        self.valList = vallist\n",
    "        self.valNum = len(vallist)\n",
    "        valarray = np.array(vallist)\n",
    "        self.valUTM = np.transpose(valarray[:,2:].astype(np.float64))\n",
    "        \n",
    "        self.testList = testlist\n",
    "        self.testNum = len(testlist)\n",
    "        testarray = np.array(testlist)\n",
    "        self.testlUTM = np.transpose(testarray[:,2:].astype(np.float64))\n",
    "        \n",
    "        fulllist = vallist+trainlist+testlist\n",
    "        self.fullList = fulllist\n",
    "        self.fullNum = len(fulllist)\n",
    "        fullarray = np.array(fulllist)\n",
    "        self.fullUTM = np.transpose(fullarray[:,2:].astype(np.float64))\n",
    "        \n",
    "        self.fullIdList = [*range(0,self.fullNum,1)]\n",
    "        self.IdList_to_use = []\n",
    "        self.__cur_id = 0 \n",
    "        self.__cur_test_id = 0\n",
    "        \n",
    "        print('For each satellite image, storing index of nearby images with the given radius. This takes some time')\n",
    "        fullUTM_transposed = np.transpose(self.fullUTM)\n",
    "        UTMTree = KDTree(fullUTM_transposed)\n",
    "        self.neighbors = {}\n",
    "        for i in range(self.fullNum):\n",
    "            center_UTM = fullUTM_transposed[i,:]\n",
    "            idx = UTMTree.query_ball_point(center_UTM,r=self.radius, p=2)\n",
    "            # exclude images at exactly same location\n",
    "            candidate = np.delete(idx, np.where(np.sum((fullUTM_transposed[idx,:]==center_UTM).astype(int),axis=1)==2))\n",
    "            self.neighbors.update({str(i):idx})\n",
    "        print('number of satellite images', self.fullNum)    \n",
    "        print('number of ground images in training set', self.trainNum)    \n",
    "        print('number of ground images in validation set', self.valNum) \n",
    "        print('number of ground images in test set', self.testNum)\n",
    "\n",
    "\n",
    "    def next_batch_scan(self, batch_size):\n",
    "        if self.__cur_test_id >= self.fullNum:\n",
    "            self.__cur_test_id = 0 # return none and reset current index to zero after scanned all elements \n",
    "            return None, None, None\n",
    "        elif self.__cur_test_id + batch_size >= self.fullNum:\n",
    "            batch_size = self.fullNum - self.__cur_test_id\n",
    "\n",
    "        batch_sat = np.zeros([batch_size, 112, 616, 3], dtype=np.float32)\n",
    "        batch_grd = np.zeros([batch_size, 112, 616, 3], dtype=np.float32)\n",
    "        batch_utm = np.zeros([batch_size, 2], dtype=np.float32)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            img_idx = self.__cur_test_id + i\n",
    "\n",
    "            # satellite, load all satellite images\n",
    "            img = cv2.imread(self.image_root + self.fullList[img_idx][1])\n",
    "            if img is None:\n",
    "                print('InputData::next_pair_batch: read fail: %s' % (self.fullList[img_idx][1]))\n",
    "                continue\n",
    "\n",
    "            img = img.astype(np.float32)\n",
    "\n",
    "            img[:, :, 0] -= 103.939  # Blue\n",
    "            img[:, :, 1] -= 116.779  # Green\n",
    "            img[:, :, 2] -= 123.6  # Red\n",
    "            batch_sat[i, :, :, :] = img\n",
    "\n",
    "            # ground, load ground image if it is in the validation set\n",
    "            if self.fullList[img_idx] in self.valList:\n",
    "                img = cv2.imread(self.image_root + self.fullList[img_idx][0])\n",
    "\n",
    "                if img is None:\n",
    "                    print('InputData::next_pair_batch: read fail: %s' % (self.fullList[img_idx][0]))\n",
    "                    continue\n",
    "                img = cv2.resize(img, (616, 112), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "                img = img.astype(np.float32)\n",
    "\n",
    "                img[:, :, 0] -= 103.939  # Blue\n",
    "                img[:, :, 1] -= 116.779  # Green\n",
    "                img[:, :, 2] -= 123.6  # Red\n",
    "                batch_grd[i, :, :, :] = img\n",
    "\n",
    "            batch_utm[i,0] = self.fullUTM[0, img_idx]\n",
    "            batch_utm[i, 1] = self.fullUTM[1, img_idx]\n",
    "\n",
    "        self.__cur_test_id += batch_size\n",
    "\n",
    "        return batch_sat, batch_grd, batch_utm\n",
    "\n",
    "\n",
    "    def next_pair_batch(self, batch_size):\n",
    "        if self.__cur_id == 0:\n",
    "            self.IdList_to_use = copy.deepcopy(self.fullIdList)\n",
    "            for i in range(20):\n",
    "                random.shuffle(self.IdList_to_use) #Only shuffle the id at the beginning of every epoch           \n",
    "       \n",
    "        \n",
    "        batch_sat = np.zeros([batch_size, 112, 616, 3], dtype=np.float32)\n",
    "        batch_grd = np.zeros([batch_size, 112, 616, 3], dtype=np.float32)\n",
    "\n",
    "        batch_idx = 0\n",
    "        i = 1\n",
    "        empty_grd = []\n",
    "        while True:\n",
    "            if self.__cur_id + batch_size >= self.fullNum:\n",
    "                self.__cur_id = 0 # return none and reset current index to zero after every epoch\n",
    "                print('This epoch finished')\n",
    "                return None, None, None, None, None\n",
    "            \n",
    "            if batch_idx >= batch_size:\n",
    "                break\n",
    "                \n",
    "            if self.__cur_id + i >= len(self.IdList_to_use):\n",
    "                # go to next center image if cannot find enough nearby images in remaining images\n",
    "                self.__cur_id += 1\n",
    "                batch_idx = 0\n",
    "                i = 1\n",
    "                continue\n",
    "            \n",
    "            if batch_idx == 0:\n",
    "            # Load the center image\n",
    "                img_idx = self.IdList_to_use[self.__cur_id]\n",
    "                \n",
    "                # Get the indexes of nearby images for current center image\n",
    "                candidates = self.neighbors[str(img_idx)]\n",
    "                \n",
    "                 # If number of nearby images is not enough to form a batch, then move to the next center image\n",
    "                if len(candidates) < batch_size-1:\n",
    "#                     print('There is no enough nearby images for current query')\n",
    "                    self.__cur_id += 1\n",
    "                    continue\n",
    "                    \n",
    "                # satellite\n",
    "                img = cv2.imread(self.image_root + self.fullList[img_idx][1])\n",
    "                if img is None:\n",
    "                    print('InputData::next_pair_batch: read fail: %s' % (self.fullList[img_idx][1]))\n",
    "                    self.__cur_id += 1\n",
    "                    continue\n",
    "                    \n",
    "                img = img.astype(np.float32)\n",
    "                img[:, :, 0] -= 103.939  # Blue\n",
    "                img[:, :, 1] -= 116.779  # Green\n",
    "                img[:, :, 2] -= 123.6  # Red\n",
    "                batch_sat[0, :, :, :] = img\n",
    "                \n",
    "                # ground\n",
    "                if self.fullList[img_idx] in self.trainList: \n",
    "                    img = cv2.imread(self.image_root + self.fullList[img_idx][0])\n",
    "                    if img is None:\n",
    "                        print('InputData::next_pair_batch: read fail: %s' % (self.fullList[img_idx][0]))\n",
    "                        self.__cur_id += 1\n",
    "                        continue\n",
    "                    img = cv2.resize(img, (616, 112), interpolation=cv2.INTER_AREA)\n",
    "                    img = img.astype(np.float32)\n",
    "                    img[:, :, 0] -= 103.939  # Blue\n",
    "                    img[:, :, 1] -= 116.779  # Green\n",
    "                    img[:, :, 2] -= 123.6  # Red\n",
    "                    batch_grd[0, :, :, :] = img\n",
    "                else:\n",
    "                    # do not load ground images if they are not in the training set\n",
    "                    empty_grd.append(batch_idx)\n",
    "                # coordinates\n",
    "                batch_utm = np.zeros([batch_size, 2], dtype=np.float32)\n",
    "                batch_utm[batch_idx,0] = self.fullUTM[0, img_idx]\n",
    "                batch_utm[batch_idx, 1] = self.fullUTM[1, img_idx]\n",
    "                \n",
    "                batch_idx += 1\n",
    "            else:\n",
    "            # Load other neaby images into the batch\n",
    "            # if the current image is in the indexes list of nearby images, read this image, and remove this index\n",
    "                if self.IdList_to_use[self.__cur_id + i] in candidates:\n",
    "                    img_idx = self.IdList_to_use[self.__cur_id + i]\n",
    "                    del self.IdList_to_use[self.__cur_id + i]\n",
    "\n",
    "                    # satellite\n",
    "                    img = cv2.imread(self.image_root + self.fullList[img_idx][1])\n",
    "                    if img is None:\n",
    "                        print('InputData::next_pair_batch: read fail: %s' % (self.fullList[img_idx][1]))\n",
    "                        i += 1\n",
    "                        continue\n",
    "\n",
    "                    img = img.astype(np.float32)\n",
    "                    # normalize it to -1 --- 1\n",
    "                    img[:, :, 0] -= 103.939  # Blue\n",
    "                    img[:, :, 1] -= 116.779  # Green\n",
    "                    img[:, :, 2] -= 123.6  # Red\n",
    "                    batch_sat[batch_idx, :, :, :] = img\n",
    "                    \n",
    "                    # ground\n",
    "                    if self.fullList[img_idx] in self.trainList:\n",
    "                        img = cv2.imread(self.image_root + self.fullList[img_idx][0])\n",
    "                        if img is None:\n",
    "                            print('InputData::next_pair_batch: read fail: %s' % (self.fullList[img_idx][0]))\n",
    "                            i += 1\n",
    "                            continue\n",
    "                        img = cv2.resize(img, (616, 112), interpolation=cv2.INTER_AREA)\n",
    "                        img = img.astype(np.float32)\n",
    "                        img[:, :, 0] -= 103.939  # Blue\n",
    "                        img[:, :, 1] -= 116.779  # Green\n",
    "                        img[:, :, 2] -= 123.6  # Red\n",
    "                        batch_grd[batch_idx, :, :, :] = img\n",
    "                    else:\n",
    "                        empty_grd.append(batch_idx)\n",
    "                    batch_utm[batch_idx,0] = self.fullUTM[0, img_idx]\n",
    "                    batch_utm[batch_idx, 1] = self.fullUTM[1, img_idx]\n",
    "\n",
    "                    batch_idx += 1\n",
    "                else:\n",
    "                    i += 1\n",
    "     \n",
    "        self.__cur_id += 1\n",
    "            \n",
    "        distance_matrix = scipy.spatial.distance_matrix(batch_utm, batch_utm)\n",
    "        # check the distance between every two images in current batch\n",
    "        useful_pairs = (distance_matrix <= self.radius).astype(np.int)\n",
    "        # diagonal elements are positive samples\n",
    "        np.fill_diagonal(useful_pairs, 0)\n",
    "        useful_pairs_s2g = copy.deepcopy(useful_pairs)\n",
    "        useful_pairs_g2s = copy.deepcopy(useful_pairs)\n",
    "        # We do not use pairs which contains empty ground images\n",
    "        for i in empty_grd:\n",
    "            useful_pairs_s2g[:,i] = 0\n",
    "            useful_pairs_s2g[i,:] = 0\n",
    "            useful_pairs_g2s[:,i] = 0 \n",
    "        \n",
    "        # We use squared distance in our Gaussian weighting term\n",
    "        squared_batch_dis = np.square(distance_matrix)\n",
    "        \n",
    "        return batch_sat, batch_grd, squared_batch_dis, useful_pairs_s2g, useful_pairs_g2s\n",
    "    \n",
    "    def get_dataset_size(self):\n",
    "        return self.trainNum\n",
    "\n",
    "    def get_test_dataset_size(self):\n",
    "        return self.valNum\n",
    "    \n",
    "    def get_full_dataset_size(self):\n",
    "        return self.fullNum\n",
    "\n",
    "    def reset_scan(self):\n",
    "        self.__cur_test_id = 0\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "network_type = \"SAFA_8\"\n",
    "start_epoch = 0\n",
    "batch_size = 4\n",
    "is_training = True\n",
    "loss_weight = 10.0\n",
    "number_of_epoch = 100\n",
    "learning_rate_val = 1e-5\n",
    "keep_prob_val = 0.8\n",
    "\n",
    "# -------------------------------------------------------- #\n",
    "\n",
    "def validate(dist_array, top_k):\n",
    "    accuracy = 0.0\n",
    "    data_amount = 0.0\n",
    "    for i in range(dist_array.shape[1]):\n",
    "        gt_dist = dist_array[i, i]\n",
    "        prediction = np.sum(dist_array[:, i] < gt_dist)\n",
    "        if prediction < top_k:\n",
    "            accuracy += 1.0\n",
    "        data_amount += 1.0\n",
    "    \n",
    "    accuracy /= data_amount\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def local_validation(dist_array, top_k, input_data):\n",
    "    accuracy = 0.0\n",
    "    data_amount = 0.0\n",
    "    for i in range(dist_array.shape[1]):\n",
    "        nearby_indexes = input_data.neighbors[str(i)]\n",
    "        gt_dist = dist_array[i, i]\n",
    "        list_withincircle = [dist_array[j, i] for j in nearby_indexes]\n",
    "        prediction = np.sum(list_withincircle<gt_dist)\n",
    "        if prediction < top_k:\n",
    "            accuracy += 1.0\n",
    "        data_amount += 1.0\n",
    "    \n",
    "    accuracy /= data_amount\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def loss_on_val_set(dist_array, input_data):\n",
    "    data_amount = 0.0\n",
    "    loss = []\n",
    "    loss_median = []\n",
    "    loss_90quantile = []\n",
    "    loss_99quantile = []\n",
    "    loss_max = []\n",
    "    for i in range(dist_array.shape[1]):\n",
    "        nearby_indexes = input_data.neighbors[str(i)]\n",
    "        gt_dist = dist_array[i, i]\n",
    "        list_withincircle = [dist_array[j, i] for j in nearby_indexes]\n",
    "        data_amount += 1.0\n",
    "        \n",
    "        triplet_dist = gt_dist - list_withincircle\n",
    "        individual_loss = np.sum(np.log(1+np.exp(triplet_dist * loss_weight))) / len(list_withincircle)\n",
    "        loss.append(individual_loss)\n",
    "        loss_median.append(np.median(np.log(1+np.exp(triplet_dist * loss_weight))))\n",
    "        loss_max.append(np.max(np.log(1+np.exp(triplet_dist * loss_weight))))\n",
    "        loss_90quantile.append(np.percentile(np.log(1+np.exp(triplet_dist * loss_weight)), 90))\n",
    "        loss_99quantile.append(np.percentile(np.log(1+np.exp(triplet_dist * loss_weight)), 99))\n",
    "    \n",
    "    loss = np.sum(loss) / data_amount    \n",
    "    loss_median = np.sum(loss_median) / data_amount\n",
    "    loss_90quantile = np.sum(loss_90quantile) / data_amount\n",
    "    loss_99quantile = np.sum(loss_99quantile) / data_amount\n",
    "    loss_max = np.sum(loss_max) / data_amount\n",
    "    \n",
    "    print('on validation set:')\n",
    "    print('loss: ', loss, ' loss_median: ', loss_median, ' loss_90quantile: ', loss_90quantile, ' loss_99quantile: ', loss_99quantile, ' loss_max: ', loss_max)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def compute_loss(sat_global, grd_global, utms_x, UTMthres, useful_pairs_s2g, useful_pairs_g2s):\n",
    "    \n",
    "    with tf.name_scope('weighted_soft_margin_triplet_loss'):\n",
    "        # Get the Gaussian weight term for every triplet\n",
    "        tfd = tfp.distributions\n",
    "        zeros = tf.fill(tf.shape(useful_pairs_s2g), 0.0)\n",
    "        sig = tf.fill(tf.shape(useful_pairs_s2g), tf.constant(UTMthres,dtype=tf.float32)) \n",
    "        dist = tfd.Normal(loc=zeros, scale=sig)\n",
    "        Gaussian_weights = (-dist.prob(utms_x)+dist.prob(zeros))/dist.prob(zeros)\n",
    "\n",
    "       \n",
    "        batch_size, channels = sat_global.get_shape().as_list()\n",
    "        dist_array = 2 - 2 * tf.matmul(sat_global, grd_global, transpose_b=True) #[S1G1, S1G2; S2G1, S2G2]\n",
    "        pos_dist = tf.diag_part(dist_array)\n",
    "        \n",
    "        # ground to satellite\n",
    "        pair_n_g2s = tf.reduce_sum(useful_pairs_g2s) + 0.001\n",
    "        triplet_dist_g2s = (pos_dist - dist_array)\n",
    "        loss_g2s = tf.reduce_sum(tf.log(1 + tf.exp(triplet_dist_g2s * loss_weight))*\\\n",
    "                                 tf.multiply(Gaussian_weights, useful_pairs_g2s)) / pair_n_g2s\n",
    "\n",
    "        # satellite to ground\n",
    "        pair_n_s2g = tf.reduce_sum(useful_pairs_s2g) + 0.001\n",
    "        triplet_dist_s2g = (tf.expand_dims(pos_dist, 1) - dist_array)\n",
    "        loss_s2g = tf.reduce_sum(tf.log(1 + tf.exp(triplet_dist_s2g * loss_weight))*\\\n",
    "                                 tf.multiply(Gaussian_weights, useful_pairs_s2g)) / pair_n_s2g\n",
    "\n",
    "        loss = (loss_g2s + loss_s2g) / 2.0\n",
    "        \n",
    "    return loss\n",
    "\n",
    "def train(start_epoch=0, radius=500):\n",
    "    '''\n",
    "    Train the network and do the test\n",
    "    :param start_epoch: the epoch id start to train. The first epoch is 0.\n",
    "    '''\n",
    "    # put your path to the model here\n",
    "    model_root = '/local/zxia/checkpoints/safa/Model'\n",
    "    \n",
    "    # import data\n",
    "    print('radius', radius)\n",
    "    input_data = InputData(radius)\n",
    "\n",
    "    # define placeholders\n",
    "    sat_x = tf.placeholder(tf.float32, [None, 112, 616, 3], name='sat_x')\n",
    "    grd_x = tf.placeholder(tf.float32, [None, 112, 616, 3], name='grd_x')\n",
    "    useful_pair_s2g_op = tf.placeholder(tf.float32, [batch_size, batch_size], name='useful_pair_s2g_op')\n",
    "    useful_pair_g2s_op = tf.placeholder(tf.float32, [batch_size, batch_size], name='useful_pair_g2s_op')\n",
    "    utms_x = tf.placeholder(tf.float32, [None, None], name='utms')\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    learning_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "    # build model\n",
    "    dimension = int(network_type[-1])\n",
    "    sat_global, grd_global = SAFA(sat_x, grd_x, keep_prob, dimension, is_training)\n",
    "\n",
    "    out_channel = sat_global.get_shape().as_list()[-1]\n",
    "    sat_global_descriptor = np.zeros([input_data.get_full_dataset_size(), out_channel])\n",
    "    grd_global_descriptor = np.zeros([input_data.get_full_dataset_size(), out_channel])\n",
    "    loss = compute_loss(sat_global, grd_global, utms_x, input_data.sig, useful_pair_s2g_op, useful_pair_g2s_op)\n",
    "\n",
    "    # set training\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    with tf.name_scope('train'):\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate, 0.9, 0.999).minimize(loss, global_step=global_step)\n",
    "\n",
    "    saver = tf.train.Saver(tf.global_variables(), max_to_keep=None)\n",
    "    \n",
    "    global_vars = tf.global_variables()\n",
    "\n",
    "    var_list = []\n",
    "    for var in global_vars:\n",
    "        if 'VGG' in var.op.name and 'Adam' not in var.op.name:\n",
    "            var_list.append(var)\n",
    "\n",
    "    saver_to_restore = tf.train.Saver(var_list)\n",
    "\n",
    "    # run model\n",
    "    print('run model...')\n",
    "    config = tf.ConfigProto(log_device_placement=False, allow_soft_placement=True)\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print('load model...')\n",
    "\n",
    "        if start_epoch == 0:\n",
    "            load_model_path = model_root + '/Initialize/initial_model.ckpt'\n",
    "        else:\n",
    "            load_model_path = model_root + '/CVACT/' + str(radius)+'/' + str(start_epoch - 1) + '/model.ckpt'\n",
    "\n",
    "        saver.restore(sess, load_model_path)\n",
    "\n",
    "        print(\"   Model loaded from: %s\" % load_model_path)\n",
    "        print('load model...FINISHED')\n",
    "\n",
    "        # Train\n",
    "        for epoch in range(start_epoch, number_of_epoch):\n",
    "            iter = 0\n",
    "            while True:\n",
    "                # train\n",
    "                batch_sat, batch_grd, squared_batch_dis, useful_pairs_s2g, useful_pairs_g2s = input_data.next_pair_batch(batch_size)\n",
    "\n",
    "                if batch_sat is None:\n",
    "                    break\n",
    "\n",
    "                global_step_val = tf.train.global_step(sess, global_step)\n",
    "\n",
    "                feed_dict = {sat_x: batch_sat, grd_x: batch_grd, utms_x:squared_batch_dis, useful_pair_s2g_op: useful_pairs_s2g,\n",
    "                             useful_pair_g2s_op: useful_pairs_g2s,\n",
    "                             learning_rate: learning_rate_val, keep_prob: keep_prob_val}\n",
    "                _, loss_val = sess.run([train_step, loss], feed_dict=feed_dict)\n",
    "\n",
    "                if iter % 20 == 0:\n",
    "                    print('global %d, epoch %d, iter %d: loss : %.8f ' % (global_step_val, epoch, iter, loss_val))\n",
    "\n",
    "                iter += 1\n",
    "\n",
    "            model_dir = model_root + '/CVACT/' +str(radius)+'/' + str(epoch) + '/'\n",
    "\n",
    "            if not os.path.exists(model_dir):\n",
    "                os.makedirs(model_dir)\n",
    "            save_path = saver.save(sess, model_dir + 'model.ckpt')\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "            # ---------------------- validation ----------------------\n",
    "            if epoch % 10 == 0:\n",
    "                print('validate...')\n",
    "                print('   compute global descriptors')\n",
    "                input_data.reset_scan()\n",
    "\n",
    "                val_i = 0\n",
    "                while True:\n",
    "                    batch_sat, batch_grd, _ = input_data.next_batch_scan(batch_size)\n",
    "                    if batch_sat is None:\n",
    "                        break\n",
    "                    feed_dict = {sat_x: batch_sat, grd_x: batch_grd, keep_prob: 1.0}\n",
    "                    sat_global_val, grd_global_val = \\\n",
    "                        sess.run([sat_global, grd_global], feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "                    sat_global_descriptor[val_i: val_i + sat_global_val.shape[0], :] = sat_global_val\n",
    "                    grd_global_descriptor[val_i: val_i + grd_global_val.shape[0], :] = grd_global_val\n",
    "                    val_i += sat_global_val.shape[0]\n",
    "\n",
    "                grd_global_descriptor_to_use = grd_global_descriptor[0:input_data.valNum,:]\n",
    "\n",
    "                print('   compute accuracy')\n",
    "                dist_array = 2 - 2 * np.matmul(sat_global_descriptor, np.transpose(grd_global_descriptor_to_use))\n",
    "\n",
    "                val_accuracy_global = np.zeros((1, 10))\n",
    "                val_accuracy_local = np.zeros((1, 10))\n",
    "                print('start')\n",
    "                for i in range(1,11):\n",
    "                    print(i)\n",
    "                    val_accuracy_global[0, i-1] = validate(dist_array, i)\n",
    "                    val_accuracy_local[0, i-1] = local_validation(dist_array, i, input_data)\n",
    "                print('epoch',  epoch, 'global accuracy on validation set =', val_accuracy_global * 100.0)\n",
    "                print('epoch',  epoch, 'local accuracy on validation set = ', val_accuracy_local * 100.0)\n",
    "                \n",
    "                loss_val = loss_on_val_set(dist_array, input_data)\n",
    "                \n",
    "                file = './' + str(radius) + '_accuracy.txt'\n",
    "                with open(file, 'a') as file:\n",
    "                    np.savetxt(file, val_accuracy_global, fmt='%4f', delimiter=',',newline='\\n', header='val_global', comments=str(epoch)+'_')\n",
    "                    np.savetxt(file, val_accuracy_local, fmt='%4f', delimiter=',',newline='\\n', header='val_local', comments=str(epoch)+'_')\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
